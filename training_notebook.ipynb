{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Notebook\n",
    "\n",
    "### First and foremost... sorry, this was not originally built with other people in mind.\n",
    "\n",
    "### General Approach\n",
    "I saw best performance from my models when I mixed augmented (skewed, cropped, rotated, etc.) photos into the data set, so my training workflow generally looked like:  \n",
    "1. Create augmented copies of all photos, and save them in their own directory with its own CSV.  \n",
    "2. Load the real data and the augmented data into a combined dataframe.  \n",
    "3. Balance the data so the count of photos with baby (values 1.0, 2.0) equals the count of photos without baby (value 0.0).  \n",
    "4. Either load the most recent model to retrain, or generate a new one.  \n",
    "5. Train 3-5 epochs at a time several times.  \n",
    "6. When I'm happy with the loss, save the model.  \n",
    "7. Infer over the entire dataset. Produce a graph of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"General setup\"\"\"\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import csv\n",
    "\n",
    "# Restrict TensorFlow to only use the first GPU\n",
    "try:\n",
    "    tf_gpus = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in tf_gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create augmented photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xaug Shape (1961, 135, 240)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate augmented copies of all photos and save to file\"\"\"\r\n",
    "\r\n",
    "datatoaugDF = pd.read_csv('data/data.csv')\r\n",
    "Xaug = []\r\n",
    "yaug = []\r\n",
    "for i in range(len(datatoaugDF.index)):\r\n",
    "    path = datatoaugDF.iloc[i]['ResizedPath']\r\n",
    "    image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\r\n",
    "    value = datatoaugDF.iloc[i]['Value']\r\n",
    "    Xaug.append(image)\r\n",
    "    yaug.append(value)\r\n",
    "\r\n",
    "Xaug = np.array(Xaug)\r\n",
    "print(f'Xaug Shape {Xaug.shape}')\r\n",
    "Xaug = Xaug/255\r\n",
    "Xaug = Xaug.reshape(-1, 135, 240,1)\r\n",
    "\r\n",
    "# Create numpy array of output values (already 0 or 1)\r\n",
    "# Transform to categorical. Use the number of classes as the dimension for the last dense layer in the model\r\n",
    "yaug = np.array(yaug)\r\n",
    "\r\n",
    "\r\n",
    "import csv\r\n",
    "import os\r\n",
    "OUTPUT_DIRECTORY_AUGMENTED = 'C:\\\\Users\\\\parki\\\\Documents\\\\GitHub\\\\Python-Practice\\\\Sleep_Schedule\\\\scripts\\\\static\\\\Augmented'\r\n",
    "\r\n",
    "#Clear existing augmented files\r\n",
    "files_list = [os.path.join(OUTPUT_DIRECTORY_AUGMENTED, file) for file in os.listdir(OUTPUT_DIRECTORY_AUGMENTED)]\r\n",
    "for file in files_list:\r\n",
    "    os.remove(file)\r\n",
    "\r\n",
    "datagen = ImageDataGenerator(\r\n",
    "    #brightness_range= [.95, 1.05], # Causes image to scale back to 0-255\r\n",
    "    rotation_range= 3,\r\n",
    "    width_shift_range= .05,\r\n",
    "    height_shift_range= .05,\r\n",
    "    zoom_range= 0.2, \r\n",
    "    shear_range= 0.1\r\n",
    ")\r\n",
    "    \r\n",
    "with open('data/augmenteddata.csv', 'w', newline='') as f:\r\n",
    "    csv.writer(f).writerow(['FilePath','ResizedPath','Value'])\r\n",
    "    \r\n",
    "    complete_cycles = 5\r\n",
    "    #Input data size = batch_size*multiple\r\n",
    "    batch_size = 53\r\n",
    "    multiple = 37\r\n",
    "\r\n",
    "    for k in range(complete_cycles):\r\n",
    "        for i in range(multiple):\r\n",
    "            img_list, value_list = datagen.flow(Xaug, yaug, batch_size= batch_size).next()\r\n",
    "            for j in range(batch_size):\r\n",
    "                img = img_list[j].reshape(135, 240,1)\r\n",
    "                img = img*255\r\n",
    "                value = value_list[j]\r\n",
    "\r\n",
    "                filename = f'augmented{str(k)+str(i)+str(j)}.png'\r\n",
    "                filepath = f'{OUTPUT_DIRECTORY_AUGMENTED}\\\\{filename}'\r\n",
    "                cv2.imwrite(filepath, img)\r\n",
    "                csv.writer(f).writerow(['', filepath, value])\r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 2 & 3: Load data, combining augmented images with real images. Balance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data counts: \n",
      "0.0    2816\n",
      "2.0     859\n",
      "1.0     629\n",
      "Name: Value, dtype: int64\n",
      "Training data counts: \n",
      "0.0    1258\n",
      "2.0     713\n",
      "1.0     545\n",
      "Name: Value, dtype: int64\n",
      "Total: 2516\n",
      "X Shape (2516, 135, 240)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load dataset, normalize and split into training and testing\"\"\"\r\n",
    "\r\n",
    "df = pd.read_csv('data/data.csv')\r\n",
    "augdf = pd.read_csv('data/augmenteddata.csv')\r\n",
    "\r\n",
    "augdf2 = augdf.loc[augdf['Value'] == 0.0]\r\n",
    "df = pd.concat([df, augdf2])\r\n",
    "\r\n",
    "dataset_value_counts = df['Value'].value_counts()\r\n",
    "print(f'Data counts: \\n{dataset_value_counts}')\r\n",
    "\r\n",
    "#Get sample dataframes of each value type\r\n",
    "min_available_dict = df['Value'].value_counts().to_dict()\r\n",
    "minimum_key, min_available = min(min_available_dict.items(), key= lambda x: x[1])\r\n",
    "\r\n",
    "\r\n",
    "if minimum_key in [1.0, 2.0]:\r\n",
    "    sample_size = min([min_available_dict.get(0.0), 2*min_available])\r\n",
    "else: sample_size = min_available\r\n",
    "\r\n",
    "if sample_size % 2:\r\n",
    "    sample_size -= 1\r\n",
    "\r\n",
    "bedEmptyDF = df.loc[df['Value'] == 0.0].sample(n= sample_size)\r\n",
    "babyInBedDF = df.loc[(df['Value'] == 1.0) | (df['Value'] == 2.0)].sample(n= sample_size)\r\n",
    "# babyInBedDF = df.loc[(df['Value'] == 1.0)].sample(n= sample_size//2)\r\n",
    "# babyAsleepDF = df.loc[(df['Value'] == 2.0)].sample(n= sample_size//2)\r\n",
    "\r\n",
    "#Combine and scramble\r\n",
    "#combinedDF = pd.concat([bedEmptyDF, babyInBedDF, babyAsleepDF])\r\n",
    "combinedDF = pd.concat([bedEmptyDF, babyInBedDF])\r\n",
    "combinedDF = combinedDF.sample(frac=1).reset_index(drop=True)\r\n",
    "training_data_counts = combinedDF['Value'].value_counts()\r\n",
    "print(f'Training data counts: \\n{training_data_counts}')\r\n",
    "print(f'Total: {combinedDF.index.size}')\r\n",
    "\r\n",
    "training_data = []\r\n",
    "for i in range(len(combinedDF.index)):\r\n",
    "    path = combinedDF.iloc[i]['ResizedPath']\r\n",
    "    image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\r\n",
    "    value = combinedDF.iloc[i]['Value']\r\n",
    "    training_data.append([image, value])\r\n",
    "X = []\r\n",
    "y = []\r\n",
    "for feature, value in training_data:\r\n",
    "    X.append(feature)\r\n",
    "    if value in [1.0, 2.0]:\r\n",
    "        y.append(1)\r\n",
    "    elif value == 0.0:\r\n",
    "        y.append(0)\r\n",
    "\r\n",
    "# Create numpy array of images, normalize values between 0 and 1\r\n",
    "# Reshape array (-1 unknown qty images, 135 pixels high, 240 pixels long, 1 value per pixel)\r\n",
    "X = np.array(X)\r\n",
    "print(f'X Shape {X.shape}')\r\n",
    "X = X/255\r\n",
    "X = X.reshape(-1, 135, 240,1)\r\n",
    "\r\n",
    "# Create numpy array of output values (already 0 or 1)\r\n",
    "# Transform to categorical. Use the number of classes as the dimension for the last dense layer in the model\r\n",
    "y = np.array(y)\r\n",
    "y_cat = to_categorical(y, num_classes=2)\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Either load the most recent model to retrain, or build a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load most recent model\"\"\"\n",
    "\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "model_name = max(os.listdir('models'))\n",
    "model_path = os.path.join(current_dir, 'models', model_name)\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build model\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (9,9), input_shape= X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size= (3,3)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size= (3,3)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss= 'binary_crossentropy',\n",
    "                optimizer= 'adam',\n",
    "                metrics= ['accuracy'])\n",
    "\n",
    "NAME = f'SLEEPING-MODEL-{int(time.time())}'\n",
    "tensorboard = TensorBoard(log_dir= f'C:\\\\logs\\\\log{NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train several epochs at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "71/71 [==============================] - 1s 21ms/step - loss: 0.0324 - accuracy: 0.9890 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "71/71 [==============================] - 1s 17ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "71/71 [==============================] - 1s 17ms/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "71/71 [==============================] - 1s 17ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "71/71 [==============================] - 1s 17ms/step - loss: 0.0081 - accuracy: 0.9965 - val_loss: 0.0052 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27318019af0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Fit model with original images\"\"\"\r\n",
    "epochs = 5\r\n",
    "model.fit(\r\n",
    "            X_train, y_train,\r\n",
    "            epochs= epochs,\r\n",
    "            validation_data=(X_test, y_test),\r\n",
    "            #callbacks= [tensorboard]\r\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: If working with an existing model, save to overwrite. Otherwise, save as a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save as new model\"\"\"\r\n",
    "import os\r\n",
    "#now = int(time.time())\r\n",
    "#model.save(f'models\\\\{now}')\r\n",
    "os.mkdir(os.path.join(os.getcwd(),'scripts', 'static', 'modelpredictions', str(now)))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save as current model\"\"\"\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Infer over the entire dataset and generate a plot of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1625635636\n",
      "Innacurate classifications:\n",
      "2 of 1488 images with the baby misclassified. Error rate 0.13%\n",
      "0 of 473 images without baby misclassified. Error rate 0.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Load most recent model and make prediction on entire dataset\"\"\"\r\n",
    "\r\n",
    "import os\r\n",
    "current_dir = os.getcwd()\r\n",
    "model_name = max(os.listdir('models'))\r\n",
    "#model_name = '1620534697'\r\n",
    "#for model_name in os.listdir('models'):\r\n",
    "#for model_name in ['1620964737', '1620963157']:\r\n",
    "    \r\n",
    "model_path = os.path.join(current_dir, 'models', model_name)\r\n",
    "model_static_path = os.path.join(current_dir, 'scripts', 'static', 'modelpredictions', model_name)\r\n",
    "prediction_file = os.path.join(current_dir, 'scripts', 'static', 'modelpredictions', model_name, 'predictions.csv')\r\n",
    "strings_file = os.path.join(current_dir, 'scripts', 'static', 'modelpredictions', model_name, 'strings.csv')\r\n",
    "summary_file = os.path.join(current_dir, 'scripts', 'static', 'modelpredictions', model_name, 'summary.txt')\r\n",
    "image_file = os.path.join(current_dir, 'scripts', 'static', 'modelpredictions', model_name, f'{model_name}graph.png')\r\n",
    "model = load_model(model_path)\r\n",
    "\r\n",
    "testDF = pd.read_csv('data/data.csv')\r\n",
    "\r\n",
    "validation_list = []\r\n",
    "for i in range(len(testDF.index)):\r\n",
    "    path = testDF.iloc[i]['ResizedPath']\r\n",
    "    image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\r\n",
    "    validation_list.append(image)\r\n",
    "\r\n",
    "V = np.array(validation_list)\r\n",
    "V = V/255\r\n",
    "V = V.reshape(-1,135,240,1)\r\n",
    "\r\n",
    "validation_results = model.predict(V)\r\n",
    "resultsDF = pd.DataFrame(validation_results, columns= ['LikelyEmpty', 'LikelyBaby'])\r\n",
    "\r\n",
    "testDF = pd.concat([testDF, resultsDF], axis=1)\r\n",
    "\r\n",
    "testDF['Value'] = testDF.apply(lambda row: 'Baby' if row.Value else 'No Baby', axis=1)\r\n",
    "testDF['Prediction'] = testDF.apply(lambda row: 'Baby' if row.LikelyBaby > row.LikelyEmpty else 'No Baby', axis=1)\r\n",
    "testDF['Incorrect'] = testDF.apply(lambda row: 1 if row.Value != row.Prediction else 0, axis=1)\r\n",
    "\r\n",
    "baby_total = testDF.loc[(testDF['Value'] == 'Baby')].index.size\r\n",
    "baby_incorrect = testDF.loc[(testDF['Value'] == 'Baby') & (testDF['Incorrect'] == 1)].index.size\r\n",
    "baby_incorrect_per = round(100 * baby_incorrect / baby_total, 2)\r\n",
    "baby_incorrect_str = f'{baby_incorrect} of {baby_total} images with the baby misclassified. Error rate {baby_incorrect_per}%'\r\n",
    "\r\n",
    "no_baby_total = testDF.loc[(testDF['Value'] == 'No Baby')].index.size\r\n",
    "no_baby_incorrect = testDF.loc[(testDF['Value'] == 'No Baby') & (testDF['Incorrect'] == 1)].index.size\r\n",
    "no_baby_incorrect_per = round(100 * no_baby_incorrect / no_baby_total, 2)\r\n",
    "no_baby_incorrect_str = f'{no_baby_incorrect} of {no_baby_total} images without baby misclassified. Error rate {no_baby_incorrect_per}%'\r\n",
    "\r\n",
    "inaccurate_counts_message = f'Innacurate classifications:\\n{baby_incorrect_str}\\n{no_baby_incorrect_str}'\r\n",
    "print(f'{model_name}\\n{inaccurate_counts_message}')\r\n",
    "\r\n",
    "stringlist = []\r\n",
    "model.summary(print_fn=lambda x: stringlist.append(x))\r\n",
    "shitty_string = '================================================================='\r\n",
    "stringlist = [line if line not in shitty_string else '======================================' for line in stringlist ]\r\n",
    "short_model_summary = \"\\n\".join(stringlist)\r\n",
    "if not os.path.exists(model_static_path):\r\n",
    "    os.mkdir(model_static_path)\r\n",
    "with open(prediction_file, 'w+', newline='') as f:\r\n",
    "    testDF.to_csv(f, index=False, header=True)\r\n",
    "\r\n",
    "with open(summary_file, 'w') as f:\r\n",
    "        f.write(short_model_summary)\r\n",
    "        \r\n",
    "accurateDF = testDF.loc[testDF['Incorrect'] == 0].reset_index()\r\n",
    "inaccurateDF = testDF.loc[testDF['Incorrect'] == 1].reset_index()\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.figure(figsize=(16,9))\r\n",
    "#Plot correct predictions in blue, incorrect in red\r\n",
    "plt.scatter(accurateDF['LikelyEmpty'], accurateDF['LikelyBaby'], alpha=.2, s=100)\r\n",
    "plt.scatter(inaccurateDF['LikelyEmpty'], inaccurateDF['LikelyBaby'], color='red', alpha=.2, s=100)\r\n",
    "\r\n",
    "#Plot mid-lines\r\n",
    "# plt.plot([0,1], [0,1], color='gray', lw=.5)\r\n",
    "# plt.plot([0,1], [1,0], color='gray', lw=.5, linestyle= 'dotted')\r\n",
    "\r\n",
    "plt.xlim([-.05,1.05])\r\n",
    "plt.ylim([-.05,1.05])\r\n",
    "plt.axis('off')\r\n",
    "plt.savefig(image_file, facecolor=(1,1,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test functions, not part of typical workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Display items output from generator. Just for testing generator settings\"\"\"\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "                                rotation_range= 3,\n",
    "                                width_shift_range= .05,\n",
    "                                height_shift_range= .05,\n",
    "                                zoom_range= 0.2, \n",
    "                                shear_range= 0.1\n",
    "                                #brightness_range= [.95, 1.05], # Causes image to scale back to 0-255\n",
    "                            )\n",
    "\n",
    "plt.figure(figsize=(25,12))\n",
    "for i in range(10):\n",
    "    img, value = datagen.flow(Xaug, yaug, batch_size=1).next()\n",
    "    img = img.reshape(135, 240,1)\n",
    "    img = img*255\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.title(value[0])\n",
    "    plt.imshow(img.astype('uint8'), cmap= 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fit model with imagegenerator.\"\"\"\n",
    "datagen = ImageDataGenerator(\n",
    "                                rotation_range= 5,\n",
    "                                #width_shift_range= .1,\n",
    "                                #height_shift_range= .1,\n",
    "                                zoom_range= 0.1, \n",
    "                                #shear_range= 0.1\n",
    "                                #brightness_range= [.95, 1.05], # Causes image to scale back to 0-255\n",
    "                            )\n",
    "epochs = 5\n",
    "\n",
    "divisible_by = [x for x in range(2,20) if X_train.shape[0] % x == 0]\n",
    "batch_size = max(divisible_by)\n",
    "print(f'Batch size: {batch_size}, {X_train.shape[0]} training images')\n",
    "\n",
    "model.fit(\n",
    "            datagen.flow(X_train, y_train, batch_size= batch_size),\n",
    "            epochs= epochs,\n",
    "            steps_per_epoch= X_train.shape[0] // batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks= [tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load all models and make prediction on entire dataset\"\"\"\n",
    "\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "#model_name = max(os.listdir('models'))\n",
    "for model_name in os.listdir('models'):\n",
    "    \n",
    "    model_path = os.path.join(current_dir, 'models', model_name)\n",
    "    prediction_file = os.path.join(current_dir, 'static', 'modelpredictions', model_name, 'predictions.csv')\n",
    "    strings_file = os.path.join(current_dir, 'static', 'modelpredictions', model_name, 'strings.csv')\n",
    "    summary_file = os.path.join(current_dir, 'static', 'modelpredictions', model_name, 'summary.txt')\n",
    "    image_file = os.path.join(current_dir, 'static', 'modelpredictions', model_name, f'{model_name}graph.png')\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    shitty_string = '================================================================='\n",
    "    stringlist = [line if line not in shitty_string else '======================================' for line in stringlist ]\n",
    "    short_model_summary = \"\\n\".join(stringlist)\n",
    "\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(short_model_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c181bd5ed8933221c509e6ef7e5b26995307278dbea8c01629af8df266345df4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('Python38')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "c181bd5ed8933221c509e6ef7e5b26995307278dbea8c01629af8df266345df4"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}